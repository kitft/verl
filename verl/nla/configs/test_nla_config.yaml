# Test configuration for NLA with tiny Gemma model

model:
  actor:
    type: "AutoModelForCausalLM"
    path: "yujiepan/gemma-2-tiny-random"
    trust_remote_code: true
    dtype: "float32"  # Use float32 for CPU testing
    # Tiny model params:
    # hidden_size: 8
    # head_dim: 2
    # intermediate_size: 16
    # num_attention_heads: 4
    # num_hidden_layers: 2
    # num_key_value_heads: 2

  critic:
    type: "AutoModelForCausalLMWithVectorValueHead"
    path: "yujiepan/gemma-2-tiny-random"  # Same base model as actor
    activation_dim: 8  # Output dimension of vector value head
    dropout: 0.1
    pooling_strategy: "last"  # How to pool sequence predictions
    trust_remote_code: true
    dtype: "float32"

tokenizer:
  path: "yujiepan/gemma-2-tiny-random"
  padding_side: "left"
  truncation_side: "right"
  max_length: 512

dataset:
  train:
    parquet_files: ["test_nla_dataset_tiny.parquet"]
    activation_dim: 8  # Match the model's hidden_size
    injection_token: "<INJECT>"
    max_length: 512
    mode: "actor"  # Train actor or critic

  eval:
    parquet_files: ["test_nla_dataset_tiny.parquet"]
    activation_dim: 8  # Match the model's hidden_size
    injection_token: "<INJECT>"
    max_length: 512
    mode: "actor"

training:
  # Actor training params
  actor:
    learning_rate: 1e-4
    batch_size: 2  # Very small for testing
    gradient_accumulation_steps: 1
    max_steps: 10  # Just a few steps for testing
    warmup_steps: 2
    weight_decay: 0.01

  # Critic training params
  critic:
    learning_rate: 5e-4
    batch_size: 2
    gradient_accumulation_steps: 1
    max_steps: 10
    warmup_steps: 2
    weight_decay: 0.01
    supervised_loss_weight: 1.0  # Weight for MSE loss

  # General training params
  num_epochs: 1
  save_steps: 5
  eval_steps: 5
  logging_steps: 1
  seed: 42
  gradient_checkpointing: false  # Disable for small model
  mixed_precision: false  # Disable for CPU testing

  # NLA specific
  injection:
    strategy: "replace"  # or "add"
    position: "prompt_end"  # Where to inject activation

  reward:
    type: "negative_mse"  # Negative MSE as reward
    temperature: 1.0  # Temperature for reward scaling
    transform: "exp"  # Reward transformation: "none", "exp", "bounded"

# Rollout configuration for generation
rollout:
  num_samples: 4
  temperature: 0.7
  top_k: 50
  top_p: 0.9
  max_new_tokens: 128
  do_sample: true

# Logging
logging:
  wandb:
    enabled: false  # Disable for testing
    project: "nla-test"
    entity: null

  tensorboard:
    enabled: true
    log_dir: "./logs"

  console:
    level: "INFO"

# Paths
output_dir: "./test_nla_output"
checkpoint_dir: "./test_nla_checkpoints"
cache_dir: "./test_nla_cache"