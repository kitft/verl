defaults:
  - ppo_trainer
  - _self_

# Full NLA GRPO training configuration with Ray infrastructure

trainer:
  device: cuda
  n_gpus_per_node: 1
  nnodes: 1
  default_hdfs_dir: "./nla_output"
  project_name: "nla_rl"
  experiment_name: "grpo_training"
  worker_module: "verl.nla.workers"
  use_tensorboard: false
  default_local_dir: "./checkpoints/${trainer.experiment_name}"
  save_freq: 100
  checkpoint:
    save_freq: ${trainer.save_freq}
    load_path: null
  total_epochs: 1
  total_training_steps: 10
  steps_per_update: 1
  ppo_mini_batch_size: 2
  ppo_epochs: 1
  gradient_checkpointing: false
  clip_ratio: 1
  value_clip: 10000000000 # this shouldn't d anything
  max_grad_norm: 1.0
  normalize_reward: false
  normalize_advantages: true
  gamma: 0.99
  lam: 0.95
  logger: ["console", "wandb"]

actor_rollout_ref:
  hybrid_engine: true
  nla: ${nla}
  model:
    path: "yujiepan/gemma-2-tiny-random"
    use_shm: false
    trust_remote_code: true
    enable_gradient_checkpointing: false
  actor:
    # Token Length Configuration for NLA:
    # Actor processes: prompt (~70 tokens typical) + response (50 tokens) per sample
    # ppo_max_token_len_per_gpu = (typical_prompt + response) * micro_batch_size
    #                           = (70 + 50) * 32 = 3,840 tokens
    # Rounded to 4500 for safety margin (allows prompts up to ~90 tokens)
    strategy: fsdp2  # fsdp2 fixes pidfd_getfd crash with SGLang weight updates
    ppo_mini_batch_size: 32
    ppo_micro_batch_size_per_gpu: 32
    ppo_max_token_len_per_gpu: 4500
    use_kl_loss: false
    optim:
      lr: 1e-5
      weight_decay: 0.01
      lr_warmup_steps: 50
  rollout:
    name: "nla_sglang"
    mode: "sync"
    n: 4  # Generate N responses per prompt for GRPO (must match nla.grpo.num_trajectories_per_prompt)
    log_prob_micro_batch_size_per_gpu: 32
    log_prob_max_token_len_per_gpu: 4500  # Match actor's ppo_max_token_len_per_gpu
    tensor_model_parallel_size: 1
    disable_radix_cache: true
    gpu_memory_utilization: 0.4
    response_length: 50
    temperature: 0.7
    top_p: 0.9
    do_sample: true
    calculate_log_probs: true
  ref:
    enable: false

critic:
  # Token Length Configuration for NLA:
  # Critic processes: response (50 tokens) + optional short prefix (~20 tokens) per sample
  # ppo_max_token_len_per_gpu = (response + prefix) * micro_batch_size
  #                           = (50 + 20) * 32 = 2,240 tokens
  # Rounded to 3000 for safety margin (roughly equivalent to actor, maybe slightly less)
  enable: true
  strategy: fsdp2  # fsdp2 fixes pidfd_getfd crash with SGLang weight updates
  ppo_micro_batch_size_per_gpu: 32
  ppo_max_token_len_per_gpu: 3000
  forward_max_token_len_per_gpu: 3000
  model:
    path: "yujiepan/gemma-2-tiny-random"
    trust_remote_code: true
    enable_gradient_checkpointing: false
  optim:
    lr: 5e-5
    weight_decay: 0.01
    lr_warmup_steps: 50

algorithm:
  adv_estimator: grpo  # Use standard GRPO (NLA critic returns MSE-based values)
  norm_adv_by_std_in_grpo: true
  use_kl_in_reward: false
  kl_ctrl:
    type: "fixed"
    value: 0.1

data:
  train_batch_size: 32
  micro_batch_size_per_gpu: 32
  val_batch_size: 32
  batch_size: 32
  max_prompt_length: 128  # Hard limit for truncation; typical prompts ~70 tokens
  max_response_length: 50  # Must match actor_rollout_ref.rollout.response_length
  shuffle: true
  train_files: ["data/testing/test_nla_dataset_tiny.parquet"]
  val_files: ["data/testing/test_nla_dataset.parquet"]
  prompt_key: "prompt"
  num_workers: 0
  valid_num_workers: 0
  dataloader_num_workers: 0

reward_model:
  enable: false
  enable_resource_pool: false

ray_kwargs:
  ray_init:
    num_cpus: 4
    num_gpus: 0  # CPU testing
    runtime_env:
      env_vars:
        TOKENIZERS_PARALLELISM: "false"
        NCCL_DEBUG: "WARN"
      excludes:
        - "*.whl"
        - "*.whl.1"
        - ".git/"
        - "__pycache__/"
        - "*.pyc"
        - ".venv/"
        - "data/testing/*.parquet"
        - "*.parquet"
  resource_pool:
    actor_rollout:
      num_gpus: 0
      num_cpus: 2
    critic:
      num_gpus: 0
      num_cpus: 2

# GRPO specific settings
grpo:
  # num_trajectories_per_prompt: 2  # NOT USED - use actor_rollout_ref.rollout.n instead
  group_normalize_advantages: true
  critic_supervised_weight: 1.0
  critic_learning_rate: 5e-5
  critic_train_epochs: 1
  reward_normalize: false
  reward_transform: "negative"
  reward_scale: 1.0
  actor_learning_rate: 1e-5
  ppo_epochs: 1
  ppo_clip_ratio: 0.2

engine_kwargs:
  sglang: {}
    # attention_chunk_size: 0  # May impact performance; explore tuning this if needed

# NLA specific settings
nla:
  activation_dim: null  # Populated at runtime from model hidden_size
  injection_position: "manual"
  injection_mode: "replace"
  injection_layer_indices: [0]
