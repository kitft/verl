# @package _global_

defaults:
  - /nla_grpo_full
  - _self_

# Multi-GPU configuration for Qwen2.5-0.5B-Instruct (4 GPUs)
#
# Resource Pool Strategy:
# Both actor_rollout and critic pools use all 4 GPUs (sequential execution, not parallel).
# This maximizes GPU utilization for small models where parallel actor/critic training
# doesn't provide significant speedup due to communication overhead.
# For larger models (7B+), consider split placement (2 GPUs actor, 2 GPUs critic).
#
# GRPO Batch Size Configuration:
# With rollout.n=4 (from nla_grpo_full), each prompt generates 4 responses.
# Total experiences = data.batch_size × rollout.n = 64 × 4 = 256
# For ONE gradient update per batch: ppo_mini_batch_size MUST = data.batch_size × rollout.n = 256
# If you change rollout.n, you MUST update ppo_mini_batch_size accordingly!

actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-0.5B-Instruct
    trust_remote_code: true
    enable_gradient_checkpointing: false
    use_shm: false
  actor:
    ppo_micro_batch_size_per_gpu: 16
    ppo_mini_batch_size: 256  # 64 prompts × 4 responses (rollout.n) = 256 for one gradient update
    optim:
      lr: 1e-5
      weight_decay: 0.001
      lr_warmup_steps: 50
    fsdp_config:
      model_dtype: bf16
  rollout:
    gpu_memory_utilization: 0.7

critic:
  output_layer_index: 10
  ppo_micro_batch_size_per_gpu: 16
  model:
    path: Qwen/Qwen2.5-0.5B-Instruct
    trust_remote_code: true
    truncate_at_layer: null  # Truncate critic at layer 10 (matching activation extraction layer)
    fsdp_config:
      model_dtype: bf16
  optim:
    lr: 1e-9
    weight_decay: 0.001
    lr_warmup_steps: 50

trainer:
  n_gpus_per_node: 4  # Use 4 GPUs on this node
  nnodes: 1  # Single node
  total_training_steps: 10000
  ppo_mini_batch_size: 256  # Must match actor.ppo_mini_batch_size
  ppo_epochs: 1
  steps_per_update: 1
  project_name: nla-rl
  experiment_name: qwen-0p5b-multigpu
  logger: ["console", "wandb"]
  resume_mode: disable
  log_val_generations: 100  # Log 10 validation samples to wandb
  test_freq: 10  # Run validation every 10 steps
  rollout_data_dir: ./rollout_outputs  # Dump rollouts to disk

ray_kwargs:
  ray_init:
    num_cpus: 16
    num_gpus: 4
  # NO resource_pool specified - use verl default behavior:
  # Both actor_rollout and critic use all 4 GPUs sequentially

algorithm:
  adv_estimator: grpo  # Use standard GRPO (NLA critic returns MSE-based values)

nla:
  injection:
    mode: replace
    layer_indices: [0]
    projection_dim: null
    injection_token: null

engine_kwargs:
  sglang:
    # attention_chunk_size: 0  # Uncomment to disable SWA if encountering NoneType errors with input_embeds

# Dataset paths generated by scripts/run_qwen_tiny_pipeline.py
# (use the canonical layer10 activations by default).
data:
  train_batch_size: 64  # Number of prompts per training step
  val_batch_size: 64
  batch_size: 64  # Kept for compatibility
  train_files:
    - /workspace/kitf/nla/data/generated/rl_train_fineweb10bt_qwen_small_layer10.parquet
  val_files:
    - /workspace/kitf/nla/data/generated/rl_val_fineweb10bt_qwen_small_layer10.parquet
