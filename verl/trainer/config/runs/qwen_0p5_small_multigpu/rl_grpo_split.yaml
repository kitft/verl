# @package _global_

defaults:
  - rl_grpo  # Inherit from base config
  - _self_

# Split Placement Configuration for Qwen2.5-0.5B-Instruct (4 GPUs)
#
# This config enables split placement where actor_rollout and critic can run
# in parallel on separate GPU pools (requires code modifications with blocking=False).
#
# Resource Allocation:
# - Actor/Rollout pool: 2 GPUs (GPUs 0-1)
# - Critic pool: 2 GPUs (GPUs 2-3)
#
# CRITICAL LIMITATION: SGLang with tensor_model_parallel_size > 1 does NOT work
# with resource pools because Ray/FSDP creates multiple worker processes per pool,
# and SGLang's TP ranks get split across different processes (they can't communicate).
#
# Solution: Use TP=1. With 2 GPUs in actor_rollout pool:
# - GPU 0: SGLang rollout (TP=1)
# - GPU 1: Actor training (FSDP can use it)
# - Both GPUs utilized, just not for TP
#
# This is a verl architectural limitation, not an NLA bug.
# See SGLANG_TP_RESOURCE_POOL_ISSUE.md for full details.

# Override resource allocation
ray_kwargs:
  ray_init:
    num_cpus: 16
    num_gpus: 4
  resource_pool:
    actor_rollout:
      num_gpus: 2  # Actor/rollout pool uses GPUs 0-1
      num_cpus: 8
    critic:
      num_gpus: 2  # Critic pool uses GPUs 2-3
      num_cpus: 8

# Configure SGLang with TP=1 (REQUIRED - see note above)
actor_rollout_ref:
  rollout:
    tensor_model_parallel_size: 1  # MUST be 1 with resource pools (SGLang limitation)
    gpu_memory_utilization: 0.7    # Can use more memory with TP=1

# Note: For split placement to enable parallel actor/critic training,
# you need to modify verl/workers/fsdp_workers.py to add blocking=False:
#   @register(dispatch_mode=Dispatch.DP_COMPUTE_PROTO, blocking=False)
#   def update_actor(self, data: DataProto): ...
#
# Without this modification, actor and critic still run sequentially.
