# @package _global_

defaults:
  - /nla_grpo_full  # Inherit from base NLA config
  - override /actor@actor_rollout_ref.actor: megatron_actor  # Use Megatron actor
  - override /critic@critic: megatron_critic  # Use Megatron critic
  - _self_

# Megatron-LM configuration for Qwen3-30B-A3B (8 GPUs)
#
# Key differences from FSDP version:
# - Uses Megatron strategy with Expert Parallelism (EP=8) for MoE models
# - Pipeline parallelism MUST be 1 (PP=1) for NLA critic
# - Different batch size handling for Megatron
# - Requires offloading even with EP=8 for 30B MoE model
#
# GRPO Batch Size Configuration:
# With rollout.n=8, each prompt generates 8 responses.
# Total experiences = data.batch_size × rollout.n = 64 × 8 = 512
# For ONE gradient update per batch: ppo_mini_batch_size = 512

actor_rollout_ref:
  model:
    path: Qwen/Qwen3-30B-A3B
    trust_remote_code: true
    enable_gradient_checkpointing: true
    use_shm: false

  actor:
    _target_: verl.workers.config.McoreActorConfig  # Use Megatron actor config
    strategy: megatron  # Use Megatron instead of FSDP
    ppo_micro_batch_size_per_gpu: 2  # Small batch for MoE model memory
    ppo_mini_batch_size: 512  # 64 prompts × 8 responses (rollout.n)

    # Megatron-specific settings
    megatron:
      # Parallelism: Use Expert Parallelism for MoE models
      tensor_model_parallel_size: 1  # TP=1 (no tensor parallelism)
      expert_model_parallel_size: 8  # EP=8 (expert parallelism across 8 GPUs)
      expert_tensor_parallel_size: 1  # ETP=1 (experts not sharded)
      pipeline_model_parallel_size: 1  # MUST be 1 for NLA
      context_parallel_size: 1
      sequence_parallel: false  # Disabled when TP=1

      # Offloading DISABLED for debugging deadlock
      param_offload: false
      grad_offload: false
      optimizer_offload: false

      # Distributed optimizer for memory efficiency
      use_distributed_optimizer: true
      use_dist_checkpointing: false

      # MBridge for MoE models
      use_mbridge: true

      # Model configuration
      override_mcore_model_config:
        use_cache: false

      override_transformer_config:
        # Activation checkpointing for memory efficiency
        recompute_granularity: "selective"
        recompute_method: null

        # MoE-specific fusion optimizations
        apply_rope_fusion: true
        masked_softmax_fusion: true
        bias_activation_fusion: true
        bias_dropout_fusion: true
        gradient_accumulation_fusion: true
        deallocate_pipeline_outputs: true
        persist_layer_norm: true
        moe_grouped_gemm: true
        moe_permute_fusion: true
        moe_token_dispatcher_type: "flex"
        moe_router_dtype: fp32
        moe_enable_deepep: true

    optim:
      lr: 1e-5
      weight_decay: 0.001
      lr_warmup_steps: 50
      override_optimizer_config:
        optimizer_offload_fraction: 1.0
        overlap_cpu_optimizer_d2h_h2d: true
        use_precision_aware_optimizer: true
        optimizer_cpu_offload: true

  rollout:
    gpu_memory_utilization: 0.8  # Use 80% of GPU memory for SGLang (model + KV cache)
    tensor_model_parallel_size: 1  # SGLang uses single TP (actor TP is separate)
    n: 8  # Generate N responses per prompt for GRPO

critic:
  _target_: verl.workers.config.McoreCriticConfig  # Use Megatron critic config
  strategy: megatron  # Use Megatron instead of FSDP
  output_layer_index: 32  # Extract from layer 32 (Qwen 30B MoE architecture)
  ppo_micro_batch_size_per_gpu: 2  # Small batch for MoE model memory

  # Megatron-specific settings
  megatron:
    # Parallelism: Use Expert Parallelism for MoE models
    tensor_model_parallel_size: 1  # TP=1
    expert_model_parallel_size: 8  # EP=8
    expert_tensor_parallel_size: 1  # ETP=1
    pipeline_model_parallel_size: 1  # MUST be 1 for NLA (intermediate layer extraction)
    context_parallel_size: 1
    sequence_parallel: false

    # Offloading for memory efficiency
    param_offload: true
    grad_offload: true
    optimizer_offload: true

    # Distributed optimizer for memory efficiency
    use_distributed_optimizer: true
    use_dist_checkpointing: false

    # MBridge for MoE models
    use_mbridge: true

    # Model configuration
    override_mcore_model_config:
      use_cache: false

    override_transformer_config:
      # Activation checkpointing
      recompute_granularity: "selective"
      recompute_method: null

      # MoE-specific fusion optimizations
      apply_rope_fusion: true
      masked_softmax_fusion: true
      bias_activation_fusion: true
      bias_dropout_fusion: true
      gradient_accumulation_fusion: true
      deallocate_pipeline_outputs: true
      persist_layer_norm: true
      moe_grouped_gemm: true
      moe_permute_fusion: true
      moe_token_dispatcher_type: "flex"
      moe_router_dtype: fp32
      moe_enable_deepep: true

  # Model config for Megatron critic
  model:
    _target_: verl.workers.config.HFModelConfig  # Use HFModelConfig for Megatron
    path: Qwen/Qwen3-30B-A3B
    trust_remote_code: true
    enable_gradient_checkpointing: true

  optim:
    lr: 1e-9
    weight_decay: 0.001
    lr_warmup_steps: 50
    override_optimizer_config:
      optimizer_offload_fraction: 1.0
      overlap_cpu_optimizer_d2h_h2d: true
      use_precision_aware_optimizer: true
      optimizer_cpu_offload: true

trainer:
  n_gpus_per_node: 8  # Use 8 GPUs on this node
  nnodes: 1  # Single node
  total_training_steps: 10000
  ppo_mini_batch_size: ${actor_rollout_ref.actor.ppo_mini_batch_size}  # Must match actor
  ppo_epochs: 1
  steps_per_update: 1
  project_name: nla-rl-q30b-megatron
  experiment_name: qwen-30b-megatron-ep8
  logger: ["console", "wandb"]
  resume_mode: disable
  log_val_generations: 100
  test_freq: 100
  rollout_data_dir: ./rollout_outputs

ray_kwargs:
  ray_init:
    num_cpus: 32
    num_gpus: 8
  # No explicit resource pool - let Ray allocate based on Megatron parallelism
  # Actor and critic both use EP=8 and share the same 8 GPUs (run sequentially)

algorithm:
  adv_estimator: grpo

nla:
  injection:
    mode: replace
    layer_indices: [0]
    projection_dim: null
    injection_token: null

engine_kwargs:
  sglang: {}

data:
  train_batch_size: 64  # Number of prompts per training step
  val_batch_size: 512
  batch_size: 64  # Kept for compatibility
  train_files:
    - data/generated/rl_train_fineweb10bt_qwen_30b_layer32.parquet
  val_files:
    - data/generated/rl_val_fineweb10bt_qwen_30b_layer32.parquet

# Megatron-specific notes:
#
# 1. Expert Parallelism (EP) vs Tensor Parallelism (TP):
#    - For MoE models like Qwen3-30B-A3B, use EP instead of TP
#    - EP=8 splits the expert layers across 8 GPUs
#    - TP=1 means no tensor parallelism (standard for MoE)
#
# 2. Pipeline Parallelism (PP):
#    - MUST be 1 for NLA critic (intermediate layer extraction)
#    - Actor embedding extraction only works on first PP stage (handled automatically)
#
# 3. Resource Allocation:
#    - 8 GPUs: actor_rollout with EP=8
#    - 8 GPUs: critic with EP=8
#    - SGLang rollout uses 1 GPU (separate from actor EP)
#
# 4. Batch Sizes:
#    - Megatron uses smaller micro_batch_size_per_gpu (2) for MoE memory
#    - Total experiences = 64 prompts × 8 responses = 512
#    - ppo_mini_batch_size = 512 for one gradient update
#
# 5. Memory Optimization:
#    - Activation checkpointing enabled (selective recompute)
#    - Distributed optimizer enabled
#    - Full offloading (param, grad, optimizer + CPU optimizer offloading)
#    - MoE fusion optimizations enabled
#    - use_mbridge=True required for MoE models
