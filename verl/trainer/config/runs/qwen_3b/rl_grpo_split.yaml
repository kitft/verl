# @package _global_

defaults:
  - rl_grpo  # Inherit from base config
  - _self_

# Split Placement Configuration for Qwen2.5-3B-Instruct (8 GPUs)
#
# This config enables split placement where actor_rollout and critic can run
# in parallel on separate GPU pools (requires code modifications with blocking=False).
#
# Resource Allocation:
# - Actor/Rollout pool: 4 GPUs (GPUs 0-3)
# - Critic pool: 4 GPUs (GPUs 4-7)
#
# CRITICAL LIMITATION: SGLang with tensor_model_parallel_size > 1 does NOT work
# with resource pools because Ray/FSDP creates multiple worker processes per pool,
# and SGLang's TP ranks get split across different processes (they can't communicate).
#
# Solution: Use TP=1. With 4 GPUs in actor_rollout pool:
# - 1 GPU: SGLang rollout (TP=1)
# - 3 GPUs: Actor training (FSDP can use them)
# - All GPUs utilized, just not for TP
#
# This is a verl architectural limitation, not an NLA bug.
# See SGLANG_TP_RESOURCE_POOL_ISSUE.md for full details.

ray_kwargs:
  ray_init:
    num_cpus: 32
    num_gpus: 8
  resource_pool:
    actor_rollout:
      num_gpus: 4
      num_cpus: 16
    critic:
      num_gpus: 4
      num_cpus: 16

# Configure SGLang with TP=1 (REQUIRED - see note above)
actor_rollout_ref:
  rollout:
    tensor_model_parallel_size: 1  # MUST be 1 with resource pools (SGLang limitation)
    gpu_memory_utilization: 0.7    # Can use more memory with TP=1
